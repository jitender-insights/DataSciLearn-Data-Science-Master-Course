{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc264fee",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "\n",
    "- **Research paper link** : https://arxiv.org/pdf/1706.03762\n",
    "- **Reference link** : https://jalammar.github.io/illustrated-transformer/\n",
    " \n",
    "                          \n",
    "                          \n",
    "## Introduction to Transformers\n",
    "\n",
    "Transformers are a type of model architecture used primarily in natural language processing (NLP) but have also found applications in other fields such as computer vision. They were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017 and have since become the backbone of many state-of-the-art models, such as BERT, GPT, and T5.\n",
    "\n",
    "## Key Concepts and Components\n",
    "\n",
    "### 1. Attention Mechanism\n",
    "\n",
    "The attention mechanism allows the model to focus on different parts of the input sequence when producing each part of the output sequence. It helps capture dependencies between words regardless of their distance in the sequence.\n",
    "\n",
    "- **Self-Attention**: Calculates the importance of each word in a sequence relative to other words in the same sequence.\n",
    "- **Scaled Dot-Product Attention**: This is the core computation in the attention mechanism, which involves three matrices: Query (Q), Key (K), and Value (V). The attention score is computed using the dot product of Q and K, scaled by the square root of the dimension of K, followed by a softmax operation.\n",
    "\n",
    "    \\[\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "    \\]\n",
    "\n",
    "### 2. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention operation, the transformer uses multiple attention heads to capture information from different representation subspaces. Each head operates independently, and their outputs are concatenated and linearly transformed.\n",
    "\n",
    "- **Example**: Imagine analyzing a sentence with multiple attention heads. One head might focus on the syntactic structure (e.g., grammatical roles), while another might focus on semantic content (e.g., entities and their relationships).\n",
    "\n",
    "### 3. Positional Encoding\n",
    "\n",
    "Since transformers do not inherently capture the order of words (as opposed to recurrent neural networks), positional encodings are added to the input embeddings to provide information about the position of each word in the sequence. This helps the model understand the sequence structure.\n",
    "\n",
    "    \\[\n",
    "    PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "    \\]\n",
    "    \\[\n",
    "    PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "    \\]\n",
    "\n",
    "### 4. Feed-Forward Networks\n",
    "\n",
    "After the attention layer, each position's output is passed through a feed-forward neural network (applied independently to each position). This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "    \\[\n",
    "    FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \\]\n",
    "\n",
    "### 5. Layer Normalization and Residual Connections\n",
    "\n",
    "To stabilize and speed up training, layer normalization is applied. Additionally, residual connections are added around each sub-layer (i.e., attention and feed-forward networks), ensuring the model can retain information from previous layers.\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "The transformer architecture is composed of an encoder and a decoder, each made up of several layers.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Each encoder layer has two main components:\n",
    "\n",
    "1. Multi-head self-attention mechanism.\n",
    "2. Feed-forward neural network.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "Each decoder layer has three main components:\n",
    "\n",
    "1. Masked multi-head self-attention mechanism (to prevent attending to future positions).\n",
    "2. Multi-head attention mechanism over the encoder's output.\n",
    "3. Feed-forward neural network.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the sentence \"The cat sat on the mat.\"\n",
    "\n",
    "1. **Input Embedding**: Convert each word into its embedding vector.\n",
    "2. **Positional Encoding**: Add positional encodings to these embeddings.\n",
    "3. **Self-Attention**: Each word will compute attention scores with every other word to understand their relationships.\n",
    "4. **Multi-Head Attention**: Perform multiple self-attention operations in parallel, capturing different aspects of the sentence.\n",
    "5. **Feed-Forward**: Pass the attended embeddings through feed-forward networks.\n",
    "6. **Repeat for Multiple Layers**: The process is repeated across several encoder and decoder layers, refining the understanding at each step.\n",
    "7. **Output**: The decoder generates the output sequence (e.g., translating the sentence into another language).\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Imagine reading a complex paragraph. Instead of reading it word by word in order, you might glance back and forth to understand the context, noting important words and their relationships. Similarly, transformers use self-attention to dynamically focus on relevant parts of the sequence, making it easier to capture long-range dependencies and complex structures.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Transformers have revolutionized NLP, leading to major advancements in:\n",
    "\n",
    "- Machine Translation (e.g., Google Translate).\n",
    "- Text Summarization (e.g., summarizing articles).\n",
    "- Question Answering (e.g., chatbots).\n",
    "- Text Generation (e.g., GPT models generating human-like text).\n",
    "\n",
    "## Summary\n",
    "\n",
    "Transformers are powerful models that rely on attention mechanisms to process sequences. By using self-attention and multi-head attention, they can capture complex dependencies without regard to the sequence's length. Their architecture, comprising encoders and decoders with positional encodings, feed-forward networks, and normalization techniques, allows them to excel in various NLP tasks, providing a flexible and robust framework for modern AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "## Overview\n",
    "\n",
    "BERT, developed by Google, is designed to understand the context of a word in a sentence by looking at both its left and right context simultaneously. It is a transformer-based model that uses only the encoder part of the transformer architecture.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Bidirectional Training**: Unlike traditional models that read text sequentially (left-to-right or right-to-left), BERT reads the entire sequence of words at once. This allows it to understand the context of a word based on both its preceding and following words.\n",
    "\n",
    "2. **Pre-training Objectives**:\n",
    "    - **Masked Language Model (MLM)**: Randomly masks some of the words in the input and tries to predict them. For example, in the sentence \"The cat sat on the [MASK],\" BERT tries to predict the masked word.\n",
    "    - **Next Sentence Prediction (NSP)**: Determines if a given pair of sentences are consecutive in the original text. This helps in understanding the relationship between sentences.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "- **Input Representation**: BERT tokenizes the sentence into subwords and adds special tokens ([CLS] at the beginning and [SEP] at the end).\n",
    "- **Masked Language Model**: Randomly masks a word and predicts it. For instance, \"The cat [MASK] on the mat.\"\n",
    "- **Next Sentence Prediction**: Given a pair of sentences, BERT predicts if the second sentence follows the first.\n",
    "\n",
    "- **Note**: The [CLS] and [SEP] are special tokens used in the BERT model (and similar transformer-based models). [CLS] stands for “classification” and is added at the beginning of each sequence to represent the entire sequence for classification tasks. [SEP] is a separator token, used to separate different sequences or segments when dealing with multiple sentences\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Text Classification**: Sentiment analysis, spam detection.\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
    "- **Question Answering**: Extracting answers from a given context.\n",
    "- **Sentence Pair Tasks**: Such as entailment and similarity.\n",
    "\n",
    "---\n",
    "\n",
    "# GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "## Overview\n",
    "\n",
    "GPT, developed by OpenAI, is designed for text generation and language modeling. Unlike BERT, GPT uses the decoder part of the transformer architecture and reads text unidirectionally (left-to-right).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Unidirectional Training**: GPT generates text by predicting the next word in a sequence based on the previous words, making it particularly suited for text generation tasks.\n",
    "2. **Pre-training and Fine-tuning**:\n",
    "    - **Pre-training**: GPT is trained on a large corpus of text to predict the next word in a sequence.\n",
    "    - **Fine-tuning**: After pre-training, GPT can be fine-tuned on specific tasks with labeled data.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "- **Input Representation**: GPT tokenizes the sentence and feeds it into the model.\n",
    "- **Text Generation**: Given the prompt \"The cat sat,\" GPT generates the next word, continuing until the sequence is complete.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Text Generation**: Writing essays, stories, or code.\n",
    "- **Chatbots**: Generating human-like responses in conversational AI.\n",
    "- **Summarization**: Creating summaries of long texts.\n",
    "- **Translation**: Translating text from one language to another.\n",
    "\n",
    "## Comparison and Intuition\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "- **BERT**: Focuses on understanding context by looking at both directions in the text, making it great for tasks requiring comprehension and context (e.g., question answering, sentiment analysis).\n",
    "- **GPT**: Focuses on generating coherent text by predicting the next word in a sequence, making it ideal for creative text generation and conversational tasks.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **BERT**: Uses the encoder part of the transformer, enabling it to understand context bidirectionally.\n",
    "- **GPT**: Uses the decoder part of the transformer, focusing on sequential generation.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **BERT**: Think of it as a model that reads a whole paragraph to understand the context before answering questions about it.\n",
    "- **GPT**: Imagine it as a storyteller that generates a story one word at a time, ensuring each new word fits the previous context.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **BERT**: Bidirectional, excels at understanding context, used for tasks like classification, NER, and question answering.\n",
    "- **GPT**: Unidirectional, excels at generating text, used for tasks like text generation, chatbots, and summarization.\n",
    "\n",
    "Both BERT and GPT leverage the transformer architecture but are tailored for different types of NLP tasks, making them complementary tools in the field of natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76144652",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
