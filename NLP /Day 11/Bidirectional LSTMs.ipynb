{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08dd997",
   "metadata": {},
   "source": [
    "## Bidirectional LSTMs\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Bidirectional Long Short-Term Memory (BiLSTM) networks are a type of recurrent neural network (RNN) architecture that can capture dependencies from both past and future contexts. This is particularly useful for tasks where context from both directions is important for understanding the data.\n",
    "\n",
    "In a standard LSTM, the network processes the input sequence in a single direction, either from the start to the end (forward direction) or from the end to the start (backward direction). This means that, at each time step, the LSTM can only consider the information from the past (or future if processed in reverse). However, in many tasks, context from both past and future is crucial.\n",
    "\n",
    "BiLSTMs address this by combining two LSTMs: one that processes the sequence forward and another that processes it backward. By doing this, the network can take into account the information from both directions at each time step, leading to a more comprehensive understanding of the data.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the task of named entity recognition (NER), where we want to identify entities like names of people, organizations, and locations in a sentence. For example, in the sentence:\n",
    "\n",
    "* \"John lives in New York.\"\n",
    "\n",
    "A standard LSTM processing from left to right might recognize \"John\" as a name but might not have enough context to confidently identify \"New York\" as a location until it processes \"York.\" Similarly, if processed from right to left, the LSTM might not recognize \"John\" until it processes \"lives.\"\n",
    "\n",
    "A BiLSTM, however, processes the sentence in both directions simultaneously:\n",
    "\n",
    "1. **Forward LSTM**: Processes the sentence as:\n",
    "   - Step 1: \"John\"\n",
    "   - Step 2: \"lives\"\n",
    "   - Step 3: \"in\"\n",
    "   - Step 4: \"New\"\n",
    "   - Step 5: \"York\"\n",
    "\n",
    "2. **Backward LSTM**: Processes the sentence as:\n",
    "   - Step 1: \"York\"\n",
    "   - Step 2: \"New\"\n",
    "   - Step 3: \"in\"\n",
    "   - Step 4: \"lives\"\n",
    "   - Step 5: \"John\"\n",
    "\n",
    "The outputs from both LSTMs are then combined at each time step. This way, the network can use context from both the start and end of the sentence to make better-informed decisions.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "Here's a simple visualization to illustrate the concept:\n",
    "\n",
    "Input Sequence: John lives in New York.\n",
    "\n",
    "Forward LSTM:\n",
    "--> John --> lives --> in --> New --> York -->\n",
    "\n",
    "Backward LSTM:\n",
    "<-- John <-- lives <-- in <-- New <-- York <--\n",
    "\n",
    "Combined (BiLSTM):\n",
    "--> John --> lives --> in --> New --> York -->\n",
    "<-- John <-- lives <-- in <-- New <-- York <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0632fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
